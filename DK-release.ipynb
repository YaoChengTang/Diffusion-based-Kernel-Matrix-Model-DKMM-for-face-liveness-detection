{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.metrics.pairwise as Ker\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import DataLoader as D\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import cv2 as cv\n",
    "import tensorflow as tf\n",
    "from finetune_alexnet_with_tensorflow.alexnet import AlexNet\n",
    "from finetune_alexnet_with_tensorflow.caffe_classes import class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cost_time(diff) :\n",
    "    second = diff % 60\n",
    "    minute = diff % 3600\n",
    "    minute = minute // 60\n",
    "    hour = diff % 86400\n",
    "    hour = hour // 3600\n",
    "    day = diff // 86400\n",
    "    format_string=\"{}d {}h:{}m:{}s\"\n",
    "    return format_string.format(day, hour, minute, second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluation(pre, ground, g_l, f_l, dataType=None, ori_labels=None):\n",
    "    \"\"\"function: evaluate the result according to the accuracy, FAR, FRR, HTER\n",
    "        args:\n",
    "            pre: (n,), prediction\n",
    "            ground: (n,), ground truth\n",
    "            g_l: int, the label of bona fide example\n",
    "            f_L: int, the label of example from attack attempt\n",
    "        return:\n",
    "            acc: accuracy\n",
    "            FAR: false accept rate\n",
    "            FRR: false reject rate\n",
    "            HTER: half total error rate\n",
    "    \"\"\"\n",
    "    acc = (ground==pre).sum() / len(ground)\n",
    "    \n",
    "    tr_index = np.where(pre==g_l)\n",
    "    fa_index = np.where(pre==f_l)\n",
    "    FAR = len(np.where(ground[tr_index]==f_l)[0])/len(np.where(ground==f_l)[0])\n",
    "    FRR = len(np.where(ground[fa_index]==g_l)[0])/len(np.where(ground==g_l)[0])\n",
    "    HTER = (FAR+FRR)/2\n",
    "    \n",
    "    return acc, FAR, FRR, HTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PCA_train(X, k):\n",
    "    \"\"\"function: calculating eigenvector from X \n",
    "        \n",
    "        args:\n",
    "            X: Data\n",
    "            k: number of component\n",
    "            \n",
    "        return:\n",
    "            eigenvector (k, origin_n_features)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"start the fitting of PCA\")\n",
    "    \n",
    "    #mean,std of each feature\n",
    "    n_samples, n_features = X.shape\n",
    "    mean = np.array([np.mean(X[:,i]) for i in range(n_features)])\n",
    "    std = np.array([np.std(X[:,i]) for i in range(n_features)])\n",
    "    #normalization\n",
    "    norm_X = np.divide(X - mean, std, where=std!=0)\n",
    "    #scatter matrix\n",
    "    scatter_matrix = np.dot(np.transpose(norm_X), norm_X)\n",
    "    #Calculate the eigenvectors and eigenvalues\n",
    "    eig_val, eig_vec = np.linalg.eig(scatter_matrix)\n",
    "    eig_pairs = [(np.abs(eig_val[i]), eig_vec[:,i]) for i in range(n_features)]\n",
    "    # sort eig_vec based on eig_val from highest to lowest\n",
    "    eig_pairs = sorted(eig_pairs, key=lambda x: x[0], reverse=True)\n",
    "    # select the top k eig_vec\n",
    "    feature=np.array([ele[1] for ele in eig_pairs[:k]])\n",
    "    feature = feature.astype(np.float32)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    cost_time = get_cost_time(end_time-start_time)\n",
    "    print(\"The time cost of PCA training: \", cost_time)\n",
    "    \n",
    "    if (feature!=feature).sum() != 0 :\n",
    "        raise Exception(\"There are some nan or inf in PCA: {}\".format((feature!=feature).sum()))\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PCA_transform(X, feature):\n",
    "    \"\"\"function: transforming X with calculated eigenvector\n",
    "        \n",
    "        args:\n",
    "            X: data to be transformed\n",
    "            feature: eigenvector\n",
    "            \n",
    "        return:\n",
    "            data after transformed and normalization, (n_sample, k)\n",
    "    \"\"\"\n",
    "    # normalization\n",
    "    n_samples, n_features = X.shape\n",
    "    mean = np.array([np.mean(X[:,i]) for i in range(n_features)])\n",
    "    std = np.array([np.std(X[:,i]) for i in range(n_features)])\n",
    "    norm_X = np.divide(X - mean, std, where=std!=0)\n",
    "#     norm_X = (X - mean) / std\n",
    "    # get new data\n",
    "    data = np.dot(norm_X, np.transpose(feature))\n",
    "    res = data\n",
    "    # normalization\n",
    "    n_samples, n_features = data.shape\n",
    "    mean = np.array([np.mean(data[:,i]) for i in range(n_features)])\n",
    "    std = np.array([np.std(data[:,i]) for i in range(n_features)])\n",
    "    res = np.divide(data - mean, std, where=std!=0)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DK(data, group_index, is_traing=False, pca=None, gamma=0.00015):\n",
    "    \"\"\"function: extract DK features of each video's frame set\n",
    "        \n",
    "        args:\n",
    "            data:   frames of all video, (n_frames, n_features)\n",
    "            group_index:   all index of each video's frames in video_set, which is a dictionary\n",
    "                        like, \"real1\":([0,1,2,...,8], label), \"fake1\":([9,10,..,32], label), ...\n",
    "            \n",
    "        return:\n",
    "            while training:\n",
    "                representation:   the DK features of each video\n",
    "                labels:   labels of each video\n",
    "                pca:   the eigenvector\n",
    "            while testing:\n",
    "                representation:   the DK features of each video\n",
    "                labels:   labels of each video\n",
    "    \"\"\"\n",
    "#     reduce the dimension\n",
    "    data = np.array([ (vec-vec.mean())/vec.std() for vec in data.transpose() ]).transpose()\n",
    "    if is_traing == True:\n",
    "        pca = PCA_train(data, k=100)\n",
    "    \n",
    "    \n",
    "#     extract DK feature and label of each video\n",
    "    representation = []\n",
    "    labels = []\n",
    "    \n",
    "    \n",
    "    for key in group_index.keys():\n",
    "        labels.append(group_index[key][1])\n",
    "        frames = np.array(data[ group_index[key][0] ])\n",
    "        \n",
    "        if (frames != frames).sum() > 0:\n",
    "            print(key, (frames != frames).sum())\n",
    "        frames = PCA_transform(frames, pca)\n",
    "        if (frames != frames).sum() > 0:\n",
    "            print(key, (frames != frames).sum())\n",
    "        frames = frames.transpose()\n",
    "        \n",
    "        res = rbf_kernel(frames, gamma=gamma)\n",
    "#         res = res[ np.triu_indices(res.shape[0]) ]\n",
    "        representation.append(res.flatten())\n",
    "    \n",
    "    if is_traing:\n",
    "        return representation, labels, pca\n",
    "    else:\n",
    "        return representation, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Save_KM_label(X, Y, labels, ker_type, mat_txt_name=None, label_txt_name=None, gamma=0.0007):\n",
    "    \"\"\"function: calculate the kernel matrix, and save it as txt_name\n",
    "    \n",
    "        args:\n",
    "            X: the first data of kernel matrix operation\n",
    "            Y: the second data of kernel matrix operation\n",
    "            labels: the labels of data\n",
    "            ker_type: which kind of kernel to use\n",
    "            mat_txt_name: the name of matrix file\n",
    "            label_txt_name: the name of labels file\n",
    "            gamma: the parameter of rbf kernel\n",
    "        \n",
    "        return:\n",
    "            the kernel matrix\n",
    "        \n",
    "    \"\"\"\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    if X.shape[1] != Y.shape[1]:\n",
    "        raise Exception(\"the second dimension of X and Y are different   \", X.shape, Y.shape)\n",
    "    if X.shape[0] != len(labels):\n",
    "        raise Exception(\"labels are not corresponding to X   \", X.shape, len(labels))\n",
    "    \n",
    "    if ker_type == \"rbf\":\n",
    "        print(\"using RBF kernel with gamma=%.6f\" % (gamma))\n",
    "        ker_matrix = rbf_kernel(X, Y, gamma=gamma)\n",
    "        \n",
    "    elif ker_type == \"linear\":\n",
    "        print(\"using linear kernel\")\n",
    "        ker_matrix = Ker.linear_kernel(X, Y)\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"Kernel Type Error\")\n",
    "    print(\"Done.   \", ker_matrix.shape)\n",
    "    \n",
    "    if mat_txt_name != None and label_txt_name != None:\n",
    "        np.savetxt(mat_txt_name, ker_matrix, fmt=\"%.9f\")\n",
    "        np.savetxt(label_txt_name, labels, fmt=\"%d\")\n",
    "        print(\"Saved - '%s' - '%s'\" % (mat_txt_name, label_txt_name) )\n",
    "    else:\n",
    "        print(\"donot save the matrix and label\")\n",
    "    \n",
    "    return ker_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Deep(group_index, g_l=1, f_l=-1, random=True):\n",
    "    \"\"\"function: compute the deep feature of a frame randomly selected from the corresponding video \n",
    "        args: \n",
    "            group_index: (the inex of frames, label, the paths of frames)\n",
    "        \n",
    "        return:\n",
    "            fea_set: the deep feature\n",
    "            labels: the labels\n",
    "    \"\"\"\n",
    "    group_keys = []\n",
    "    labels = []\n",
    "    paths = []\n",
    "    for key in group_index.keys():\n",
    "        if len(group_index[key][0]) == 0:\n",
    "            print(\"There is no faces in \", key)\n",
    "            continue\n",
    "        \n",
    "        # get the label\n",
    "        labels.append(group_index[key][1])\n",
    "        \n",
    "        # randomly select a image's path\n",
    "        tmp_path_set = group_index[key][2]\n",
    "        if random is True :\n",
    "            path = tmp_path_set[np.random.randint(0, len(tmp_path_set))]\n",
    "            paths.append( path.strip() )\n",
    "        else :\n",
    "            for path in tmp_path_set :\n",
    "                paths.append( path.strip() )\n",
    "        group_keys.append(key)\n",
    "    \n",
    "    \n",
    "    # create the graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    #mean of imagenet dataset in BGR\n",
    "    imagenet_mean = np.array([104., 117., 124.], dtype=np.float32)\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "        #placeholder for input and dropout rate\n",
    "        x = tf.placeholder(tf.float32, [1, 227, 227, 3])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        #create model with default config ( == no skip_layer and 1000 units in the last layer)\n",
    "        model = AlexNet(x, keep_prob, 2, [])\n",
    "\n",
    "        #define activation of last layer as score\n",
    "        score = model.fc8\n",
    "        fea = model.fc7\n",
    "\n",
    "        #create op to calculate softmax \n",
    "        softmax = tf.nn.softmax(score)\n",
    "    \n",
    "    # saver=tf.train.import_meta_graph('./finetune_alexnet_with_tensorflow/log/checkpoints/model_epoch10.ckpt.meta')\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    \n",
    "    config = tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    \n",
    "    fea_set = []\n",
    "    pre_set = []\n",
    "    with tf.Session(config=config) as sess:\n",
    "        # initialize the variables\n",
    "        init_op = tf.group(\n",
    "                tf.global_variables_initializer(),\n",
    "                tf.local_variables_initializer()\n",
    "                )\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        # restore the weights of pretrained network\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('./finetune_alexnet_with_tensorflow/log/checkpoints-REPLAY-hsv/'))\n",
    "        \n",
    "        # Loop over all images\n",
    "        for i, path in enumerate(paths):\n",
    "            # read the image for color image\n",
    "#             image = cv.imread(path)\n",
    "            image = cv.imread(path, cv.IMREAD_GRAYSCALE)\n",
    "            image = np.stack((image,image,image), axis=2)\n",
    "            \n",
    "            # Convert image to float32 and resize to (227x227)\n",
    "            img = cv.resize(image.astype(np.float32), (227,227))\n",
    "            \n",
    "            # Subtract the ImageNet mean\n",
    "            img -= imagenet_mean\n",
    "            \n",
    "            # Reshape as needed to feed into model\n",
    "            img = img.reshape((1,227,227,3))\n",
    "\n",
    "            # Run the session and calculate the class probability\n",
    "            probs, res_fea= sess.run([softmax, fea], feed_dict={x: img, keep_prob: 1})\n",
    "            \n",
    "            # Get the class name of the class with the highest probability\n",
    "            class_name = np.argmax(probs)\n",
    "            \n",
    "            # store the feature from fc7 and the predicted class\n",
    "            fea_set.append(res_fea.flatten())\n",
    "            pre_set.append(class_name)\n",
    "            \n",
    "            if np.mod((i+1), 100) == 0 :\n",
    "                print(\"\\riteration: {}\".format(i+1), end=\"\")\n",
    "    \n",
    "#     fea_set = np.array(fea_set)\n",
    "#     pre_set = np.array(pre_set)\n",
    "#     labels = np.array(labels)\n",
    "#     pre_set[ np.where(pre_set==0) ] = f_l\n",
    "#     acc, FAR, FRR, HTER = evaluation(pre_set, labels, g_l=g_l, f_l=f_l, dataType=None, ori_labels=None)\n",
    "#     print(\"acc: {}, FAR: {}, FRR: {}, HTER: {}\".format(acc, FAR, FRR, HTER))\n",
    "    \n",
    "    return fea_set, labels, group_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_deepFea(deep_fea, group_index) :\n",
    "    res_list = []\n",
    "    for key in group_index.keys():\n",
    "#         print(type(group_index[key][0]), group_index[key][0][:10])\n",
    "        fea_set = np.array(deep_fea[ group_index[key][0] ])\n",
    "        res_list.append(fea_set.mean(axis=0))\n",
    "    \n",
    "    return np.array(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_path = \"./Data/CASIA_train_HSV_64_15diff.pkl\"\n",
    "test_path = \"./Data/CASIA_test_HSV_64_15diff.pkl\"\n",
    "# train_path = \"./Data/REPLAY_train_HSV_64_15diff.pkl\"\n",
    "# test_path = \"./Data/REPLAY_test_HSV_64_15diff.pkl\"\n",
    "# train_path = \"./Data/NUAA_train_GRAY_64_15diff_frame.pkl\"\n",
    "# test_path = \"./Data/NUAA_test_GRAY_64_15diff_frame.pkl\"\n",
    "\n",
    "saver = D.Saver(train_path)\n",
    "train_images_set, train_group_index = saver.load()\n",
    "train_images_set = train_images_set.astype(np.float64)\n",
    "\n",
    "saver = D.Saver(test_path)\n",
    "test_images_set, test_group_index = saver.load()\n",
    "test_images_set = test_images_set.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bona fide example\n",
    "g_l=1\n",
    "# artefact\n",
    "f_l=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_represetation = None\n",
    "test_represetation = None\n",
    "gamma = 0.00015\n",
    "count = 0\n",
    "for train_images, test_images in zip(np.transpose(train_images_set, (3,0,1,2)),\\\n",
    "                                     np.transpose(test_images_set, (3,0,1,2))) :\n",
    "    # flatten the image\n",
    "    train_images = np.array([img.flatten() for img in train_images])\n",
    "    test_images = np.array([img.flatten() for img in test_images])\n",
    "    print(\"After flattening, the shape of traing images: {}, the shape of testing images :{}\".format(train_images.shape,\\\n",
    "                                                                                                  test_images.shape))\n",
    "\n",
    "    # compute the DK feature from training set\n",
    "    tmp_train_represetation, train_labels, pca = DK(train_images, train_group_index, is_traing=True, gamma=gamma)\n",
    "    train_labels = np.array(train_labels)\n",
    "    print(np.array(tmp_train_represetation).shape, np.array(train_labels).shape)\n",
    "\n",
    "    # compute the DK feature from testing set\n",
    "    tmp_test_represetation, test_labels = DK(test_images, test_group_index, is_traing=False, pca=pca, gamma=gamma)\n",
    "    test_labels = np.array(test_labels)\n",
    "    print(np.array(tmp_test_represetation).shape, np.array(test_labels).shape)\n",
    "\n",
    "    if train_represetation is None :\n",
    "        train_represetation = np.array(tmp_train_represetation)\n",
    "        test_represetation = np.array(tmp_test_represetation)\n",
    "    else :\n",
    "        train_represetation = np.hstack((train_represetation, np.array(tmp_train_represetation)))\n",
    "        test_represetation = np.hstack((test_represetation, np.array(tmp_test_represetation)))\n",
    "    print(\"the shape of traing pre: {}, the shape of testing pre :{}\".format(train_represetation.shape,\\\n",
    "                                                                             test_represetation.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_DK_KM = Save_KM_label(train_represetation, train_represetation, train_labels, \"linear\")\n",
    "test_DK_KM = Save_KM_label(test_represetation, train_represetation, test_labels, \"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for the training of SVM\n",
    "svm_new = SVC(C=7, kernel=\"precomputed\", class_weight='balanced')\n",
    "print(train_DK_KM.shape, len(train_labels))\n",
    "svm_new.fit(train_DK_KM, train_labels)\n",
    "y_true, y_pred = train_labels, svm_new.predict(train_DK_KM)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(len(y_true), \"   %d - genuine, %d - fake\" % (len(np.where(y_true==1)[0]), len(np.where(y_true==-1)[0]) ))\n",
    "print(\"accuracy: \", ((y_true==y_pred).sum())/len(y_true) )\n",
    "\n",
    "\n",
    "# predict with trainging set\n",
    "y_true, y_pred = test_labels, svm_new.predict(test_DK_KM)\n",
    "# print(classification_report(y_true, y_pred))\n",
    "# y_true, y_pred = list(y_true), list(y_pred)\n",
    "print(len(y_true), \"   %d - genuine, %d - fake\" % (len(np.where(y_true==1)[0]), len(np.where(y_true==-1)[0]) ))\n",
    "y_true = np.array(y_true)\n",
    "print(\"accuracy: \", ((y_true==y_pred).sum())/len(y_true) )\n",
    "\n",
    "\n",
    "# predict with testing set\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "tr_index = np.where(y_true==g_l)\n",
    "fa_index = np.where(y_true==f_l)\n",
    "FRR = len(np.where(y_pred[tr_index]==f_l)[0])/len(tr_index[0])\n",
    "FAR = len(np.where(y_pred[fa_index]==g_l)[0])/len(fa_index[0])\n",
    "HTER = (FAR+FRR)/2\n",
    "print(\"FAR: %f   FRR: %f   HTER: %f\" % (FAR, FRR, HTER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "# get the deep features\n",
    "train_deep, train_deep_labels, train_deep_group_keys = Deep(train_group_index, g_l=g_l, f_l=f_l, random=True)\n",
    "test_deep, test_deep_labels, test_deep_group_keys = Deep(test_group_index, g_l=g_l, f_l=f_l, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_deep = np.array(train_deep)\n",
    "test_deep = np.array(test_deep)\n",
    "train_deep_labels = np.array(train_deep_labels)\n",
    "test_deep_labels = np.array(test_deep_labels)\n",
    "print(train_deep.shape, test_deep.shape)\n",
    "print(train_deep_labels.shape, test_deep_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_train_deep = train_deep\n",
    "tmp_test_deep = test_deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the average of deep features\n",
    "tmp_train_deep = average_deepFea(train_deep, train_group_index)\n",
    "tmp_test_deep = average_deepFea(test_deep, test_group_index)\n",
    "print(tmp_train_deep.shape, tmp_test_deep.shape)\n",
    "print(train_deep_labels.shape, test_deep_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_deep_KM = Save_KM_label(tmp_train_deep, tmp_train_deep, train_deep_labels, \"rbf\", gamma=5e-5)\n",
    "test_deep_KM = Save_KM_label(tmp_test_deep, tmp_train_deep, test_deep_labels, \"rbf\", gamma=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bona fide example\n",
    "g_l=1\n",
    "# artefact\n",
    "f_l=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for the training of SVM\n",
    "svm_new = SVC(C=1, kernel=\"precomputed\", class_weight={1:10})\n",
    "print(train_deep_KM.shape, len(train_deep_labels))\n",
    "svm_new.fit(train_deep_KM, train_deep_labels)\n",
    "y_true, y_pred = train_deep_labels, svm_new.predict(train_deep_KM)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(len(y_true), \"   %d - genuine, %d - fake\" % (len(np.where(y_true==1)[0]), len(np.where(y_true==-1)[0]) ))\n",
    "print(\"accuracy: \", ((y_true==y_pred).sum())/len(y_true) )\n",
    "\n",
    "\n",
    "# predict with trainging set\n",
    "y_true, y_pred = test_deep_labels, svm_new.predict(test_deep_KM)\n",
    "# print(classification_report(y_true, y_pred))\n",
    "# y_true, y_pred = list(y_true), list(y_pred)\n",
    "print(len(y_true), \"   %d - genuine, %d - fake\" % (len(np.where(y_true==1)[0]), len(np.where(y_true==-1)[0]) ))\n",
    "y_true = np.array(y_true)\n",
    "print(\"accuracy: \", ((y_true==y_pred).sum())/len(y_true) )\n",
    "\n",
    "\n",
    "# predict with testing set\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "tr_index = np.where(y_true==g_l)\n",
    "fa_index = np.where(y_true==f_l)\n",
    "FRR = len(np.where(y_pred[tr_index]==f_l)[0])/len(tr_index[0])\n",
    "FAR = len(np.where(y_pred[fa_index]==g_l)[0])/len(fa_index[0])\n",
    "HTER = (FAR+FRR)/2\n",
    "print(\"FAR: %f   FRR: %f   HTER: %f\" % (FAR, FRR, HTER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DK_nor = np.trace(train_DK_KM) / (len(train_DK_KM) * len(train_DK_KM))\n",
    "deep_nor = np.trace(train_deep_KM) / (len(train_deep_KM) * len(train_deep_KM))\n",
    "print(DK_nor, deep_nor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp1 = 0.5\n",
    "tmp2 = 1e-3\n",
    "coef1 = tmp1/(tmp1+tmp2)\n",
    "coef2 = tmp2/(tmp1+tmp2)\n",
    "coef1 = 0.98\n",
    "coef2 = 0.02\n",
    "print(coef1, coef2)\n",
    "train_KM = train_DK_KM/DK_nor*coef1 + train_deep_KM/deep_nor*coef2\n",
    "test_KM = test_DK_KM/DK_nor*coef1 + test_deep_KM/deep_nor*coef2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for the training of SVM\n",
    "svm_new = SVC(C=1, kernel=\"precomputed\", class_weight={1:10})\n",
    "print(train_KM.shape, len(train_labels))\n",
    "svm_new.fit(train_KM, train_labels)\n",
    "y_true, y_pred = train_labels, svm_new.predict(train_KM)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(len(y_true), \"   %d - genuine, %d - fake\" % (len(np.where(y_true==1)[0]), len(np.where(y_true==-1)[0]) ))\n",
    "print(\"accuracy: \", ((y_true==y_pred).sum())/len(y_true) )\n",
    "\n",
    "\n",
    "# predict with trainging set\n",
    "y_true, y_pred = test_labels, svm_new.predict(test_KM)\n",
    "# print(classification_report(y_true, y_pred))\n",
    "# y_true, y_pred = list(y_true), list(y_pred)\n",
    "print(len(y_true), \"   %d - genuine, %d - fake\" % (len(np.where(y_true==1)[0]), len(np.where(y_true==-1)[0]) ))\n",
    "y_true = np.array(y_true)\n",
    "print(\"accuracy: \", ((y_true==y_pred).sum())/len(y_true) )\n",
    "\n",
    "\n",
    "# predict with testing set\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "tr_index = np.where(y_true==g_l)\n",
    "fa_index = np.where(y_true==f_l)\n",
    "FRR = len(np.where(y_pred[tr_index]==-1)[0])/len(tr_index[0])\n",
    "FAR = len(np.where(y_pred[fa_index]==1)[0])/len(fa_index[0])\n",
    "HTER = (FAR+FRR)/2\n",
    "print(\"FAR: %f   FRR: %f   HTER: %f\" % (FAR, FRR, HTER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36-tf36-keras",
   "language": "python",
   "name": "tf36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
